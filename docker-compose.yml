version: "3.9"

services:

  ingestion:
    build: ./ingestion-service
    container_name: ingestion-service
    ports:
      - "8001:8001"
    volumes:
      - ./data/ingestion:/app/data
    depends_on:
      embedding:
        condition: service_healthy
    networks:
      - rag-network

  embedding:
    build: ./embedding-service
    container_name: embedding-service
    ports:
      - "8002:8002"
    environment:
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
      - VECTORDB_URL=http://vectordb:8003
    depends_on:
      vectordb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8002/health', timeout=2)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s
    networks:
      - rag-network

  vectordb:
    build: ./vectordb-service
    container_name: vectordb-service
    ports:
      - "8003:8003"
    volumes:
      - ./data/chromadb:/app/chromadb
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8003/health', timeout=2)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - rag-network

  query:
    build: ./query-service
    container_name: query-service
    ports:
      - "8004:8004"
    environment:
      - EMBEDDING_SERVICE_URL=http://embedding:8002
      - VECTORDB_SERVICE_URL=http://vectordb:8003
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3
    depends_on:
      embedding:
        condition: service_healthy
      vectordb:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - rag-network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        # Start Ollama in the background
        /bin/ollama serve &
        # Wait for Ollama to be ready
        until ollama list >/dev/null 2>&1; do
          echo "Waiting for Ollama to start..."
          sleep 1
        done
        # Only pull if model doesn't exist
        if ! ollama list | grep -q "llama3"; then
          echo "Model not found. Pulling llama3..."
          ollama pull llama3
          echo "Model downloaded successfully"
        else
          echo "Model llama3 already exists, skipping download"
        fi
        echo "Ollama is ready"
        # Keep container running
        wait
    networks:
      - rag-network

networks:
  rag-network:
    driver: bridge